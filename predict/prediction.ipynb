{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "from collections import Counter\n",
    "from autocorrect import Speller  # For language correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters matching your training pipeline\n",
    "BODY_KEYPOINTS = 33 * 3  # (x, y, z) for each body keypoint\n",
    "HAND_KEYPOINTS = 42 * 3  # (x, y, z) for each hand keypoint\n",
    "MAX_LEN = BODY_KEYPOINTS + HAND_KEYPOINTS  # should be 126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "def sum_over_time(x):\n",
    "    return tf.keras.backend.sum(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model\n",
    "model = load_model(\"../model/lstm_cnn_model.keras\", custom_objects={\"sum_over_time\": sum_over_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label mapping (assuming it was saved as a numpy array)\n",
    "label_classes = np.load(\"../label_encoder.npy\", allow_pickle=True)\n",
    "# Create a mapping from index to label string.\n",
    "idx2label = {i: label for i, label in enumerate(label_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mediapipe pose and hands detectors\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=2,\n",
    "                       min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Normalize keypoints from a list of dictionaries\n",
    "def normalize_keypoints(body, hands_kps):\n",
    "    \"\"\"\n",
    "    Convert the detected keypoints into a fixed-length array.\n",
    "    \n",
    "    Args:\n",
    "        body (list): List of body keypoints dictionaries, each with keys \"x\", \"y\", \"z\".\n",
    "        hands_kps (list): List of hand keypoints dictionaries, each with keys \"x\", \"y\", \"z\".\n",
    "    \n",
    "    Returns:\n",
    "        np.array: 1D numpy array of length MAX_LEN.\n",
    "    \"\"\"\n",
    "    keypoints = []\n",
    "    \n",
    "    # Process body keypoints (expected 33)\n",
    "    if body and len(body) > 0:\n",
    "        for point in body:\n",
    "            keypoints.extend([point.get(\"x\", 0.0), point.get(\"y\", 0.0), point.get(\"z\", 0.0)])\n",
    "    else:\n",
    "        keypoints.extend([0] * BODY_KEYPOINTS)\n",
    "        \n",
    "    # Process hand keypoints (expected 42)\n",
    "    if hands_kps and len(hands_kps) > 0:\n",
    "        for point in hands_kps:\n",
    "            keypoints.extend([point.get(\"x\", 0.0), point.get(\"y\", 0.0), point.get(\"z\", 0.0)])\n",
    "    else:\n",
    "        keypoints.extend([0] * HAND_KEYPOINTS)\n",
    "    \n",
    "    # Ensure the keypoints vector is of fixed length\n",
    "    if len(keypoints) < MAX_LEN:\n",
    "        keypoints += [0] * (MAX_LEN - len(keypoints))\n",
    "    elif len(keypoints) > MAX_LEN:\n",
    "        keypoints = keypoints[:MAX_LEN]\n",
    "    \n",
    "    # Optionally, you can add normalization here (e.g., min-max scaling).\n",
    "    # For simplicity, we'll assume the raw keypoints are acceptable.\n",
    "    return np.array(keypoints, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract keypoints from a frame using Mediapipe.\n",
    "# This function now returns (normalized_keypoints, hand_present_flag)\n",
    "def extract_keypoints_from_frame(frame):\n",
    "    # Convert the frame to RGB\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process with pose\n",
    "    pose_results = pose.process(image_rgb)\n",
    "    body_keypoints = []\n",
    "    if pose_results.pose_landmarks:\n",
    "        for lm in pose_results.pose_landmarks.landmark:\n",
    "            body_keypoints.append({\n",
    "                \"x\": lm.x,\n",
    "                \"y\": lm.y,\n",
    "                \"z\": lm.z\n",
    "            })\n",
    "    \n",
    "    # Process with hands\n",
    "    hands_results = hands.process(image_rgb)\n",
    "    hands_keypoints = []\n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                hands_keypoints.append({\n",
    "                    \"x\": lm.x,\n",
    "                    \"y\": lm.y,\n",
    "                    \"z\": lm.z\n",
    "                })\n",
    "    \n",
    "    # If no hand keypoints are detected, mark hand_present as False.\n",
    "    hand_present = True if hands_keypoints and len(hands_keypoints) > 0 else False\n",
    "    \n",
    "    normalized = normalize_keypoints(body_keypoints, hands_keypoints)\n",
    "    return normalized, hand_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Variables for Word and Sentence Formation (if needed) ---\n",
    "window_size = 10                     # Number of frames for smoothing\n",
    "prediction_buffer = []               # Buffer to hold recent predictions for majority vote\n",
    "current_word = \"\"                    # Current word being formed\n",
    "sentence = \"\"                        # Full sentence\n",
    "no_keypoint_count = 0                # Count of consecutive frames with \"No Keypoints\"\n",
    "no_keypoint_threshold = 15           # If exceeded, treat as a break between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Starting real-time prediction. Press 'q' to exit.\")\n",
    "prev_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Optionally, flip the image for a mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Extract keypoints and hand flag from the frame\n",
    "    keypoints_vector, hand_present = extract_keypoints_from_frame(frame)\n",
    "    \n",
    "    # If no hand is present, skip prediction (or display a message)\n",
    "    if not hand_present:\n",
    "        predicted_label = \"No Hand Detected\"\n",
    "        # Reset prediction buffer if desired\n",
    "        prediction_buffer = []\n",
    "    else:\n",
    "        # Reshape to match the model's expected input shape: (1, MAX_LEN, 1)\n",
    "        input_data = keypoints_vector.reshape(1, MAX_LEN, 1)\n",
    "        pred_prob = model.predict(input_data)\n",
    "        pred_class = np.argmax(pred_prob, axis=1)[0]\n",
    "        current_prediction = idx2label.get(pred_class, \"Unknown\")\n",
    "        \n",
    "        # Append prediction to the buffer\n",
    "        prediction_buffer.append(current_prediction)\n",
    "        if len(prediction_buffer) > window_size:\n",
    "            prediction_buffer.pop(0)\n",
    "        \n",
    "        # Compute majority vote over the buffer for smoothing\n",
    "        predicted_label = Counter(prediction_buffer).most_common(1)[0][0]\n",
    "    \n",
    "    # Calculate FPS\n",
    "    current_time = time.time()\n",
    "    fps = 1 / (current_time - prev_time)\n",
    "    prev_time = current_time\n",
    "    \n",
    "    # Display prediction and FPS on the frame\n",
    "    cv2.putText(frame, f\"Prediction: {predicted_label}\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 70), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Real-Time Prediction\", frame)\n",
    "    \n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
